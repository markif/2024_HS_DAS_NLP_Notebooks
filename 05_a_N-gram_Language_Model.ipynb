{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"400\" src=\"https://www.fhnw.ch/de/++theme++web16theme/assets/media/img/fachhochschule-nordwestschweiz-fhnw-logo.svg\" alt=\"FHNW Logo\">\n",
    "\n",
    "\n",
    "# N-gram Language Model\n",
    "\n",
    "by Fabian MÃ¤rki\n",
    "\n",
    "## Summary\n",
    "The aim of this notebook is to build a n-gram Language Model and generate text using this model. Please note: Nowadays there are more efficient methods available to build a Language Model. This notebook is meant for for illustrative purposes only.\n",
    "\n",
    "## Links\n",
    "- [N-gram Language Models](https://nlpforhackers.io/language-models)\n",
    "- [NLTK Language Modeling Module](https://kite.com/python/docs/nltk.lm)\n",
    "- [Kaggle on N-gram Language Model with NLTK](https://www.kaggle.com/alvations/n-gram-language-model-with-nltk)\n",
    "\n",
    "This notebook contains assigments: <font color='red'>Questions are written in red.</font>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/markif/2024_HS_DAS_NLP_Notebooks/blob/master/05_a_N-gram_Language_Model.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "from collections import Counter\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a LM on the Reuter corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 1720901\n",
      "Total number of unique words: 41600\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words:\", len(reuters.words()))\n",
    "print(\"Total number of unique words:\", len(counts.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK offers bigram and trigram functionalities..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = '<s>'\n",
    "end_symbol = '</s>'\n",
    "start_end_symbols = [start_symbol, end_symbol]\n",
    "n_gram = 3\n",
    "\n",
    "first_sentence = reuters.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Build bigrams\n",
    "print (list(bigrams(first_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Build trigrams\n",
    "print (list(trigrams(first_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can collect our counts in order to build a trigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a (self made) trigram Model using the [pad_both_ends](https://www.nltk.org/api/nltk.lm.html#module-nltk.lm.preprocessing) and [trigrams](https://kite.com/python/docs/nltk.trigrams) function from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.19 s, sys: 245 ms, total: 7.43 s\n",
      "Wall time: 7.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trigramModel = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for sentence in reuters.sents():\n",
    "    sentence = pad_both_ends(sentence, n=n_gram, left_pad_symbol=start_symbol, right_pad_symbol=end_symbol)\n",
    "    for w1, w2, w3 in trigrams(sentence):\n",
    "        trigramModel[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have some look on sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "8839\n"
     ]
    }
   ],
   "source": [
    "print(trigramModel[\"what\", \"the\"][\"economists\"])\n",
    "print(trigramModel[\"what\", \"the\"][\"nonexistingword\"])\n",
    "print(trigramModel[start_symbol, start_symbol][\"The\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1_w2 in trigramModel:\n",
    "    total_count = sum(trigramModel[w1_w2].values())\n",
    "    for w3 in trigramModel[w1_w2]:\n",
    "        trigramModel[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043478260869565216\n",
      "0.0\n",
      "0.16155800478879934\n"
     ]
    }
   ],
   "source": [
    "print (trigramModel[\"what\", \"the\"][\"economists\"])\n",
    "print (trigramModel[\"what\", \"the\"][\"nonexistingword\"] )\n",
    "print (trigramModel[start_symbol, start_symbol][\"The\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**TASK: Play with the random parameter `r` in order to genereate more/less diverse/coherent texts.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The department said .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "  \n",
    "text = [start_symbol, start_symbol]\n",
    " \n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    #random.seed(40)\n",
    "    r = random.random()\n",
    "    #r = random.uniform(0.0, 0.3)\n",
    "    \n",
    "    accumulator = .0\n",
    " \n",
    "    for word in trigramModel[tuple(text[-(n_gram-1):])].keys():\n",
    "        accumulator += trigramModel[tuple(text[-(n_gram-1):])][word]\n",
    "         \n",
    "        # bring in some randomness by not just taking the first word \n",
    "        # (the one with the highest probability) but sum the word's\n",
    "        # probabilities until the sum is >= the randomly drawn value\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    " \n",
    "    sentence_finished = text[-(n_gram-1):] == [end_symbol, end_symbol]\n",
    " \n",
    "print(' '.join([t for t in text if t not in start_end_symbols]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds almost like real text (at least sometimes)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-gram Language Model\n",
    "\n",
    "Build a 4-gram Language Model by training a Maximum Likelihood Estimator [MLE](https://kite.com/python/docs/nltk.lm.MLE). You can get inspiration from [here](https://kite.com/python/docs/nltk.lm).\n",
    "\n",
    "Alternatively to the Reuters corpus, you might want to consider training a model with an alternative corpus (e.g. Shakespeare from the [Gutenberg corpus](http://www.nltk.org/book_1ed/ch02.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Note:</strong> For more sophisticated n-gram models, take a look at [these objects from nltk.lm.models](https://github.com/nltk/nltk/blob/develop/nltk/lm/models.py):\n",
    "\n",
    "- Laplace: Implements Laplace (add one) smoothing.\n",
    "- Lidstone: Provides Lidstone-smoothed scores.\n",
    "- WittenBellInterpolated: Interpolated version of Witten-Bell smoothing.\n",
    "- KneserNeyInterpolated: Interpolated version of Kneser-Ney smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data, tokenize the sentences and build the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "\n",
    "tokenized_text = reuters.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for n-grams language modelling\n",
    "\n",
    "train_data, padded_sents = padded_everygram_pipeline(n_gram, tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a n-gram Model\n",
    "\n",
    "Having prepared our data we are ready to start training a model using an instance of Maximum Likelihood Estimator (MLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLE(n_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using its fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 635 ms, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 41602 items>\n"
     ]
    }
   ],
   "source": [
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you might want to ignore words that did occur during training but did not occur frequently enough to provide useful information. You can tell the vocabulary to ignore such words using the unk_cutoff argument of [nltk.lm.vocabulary.Vocabulary](https://github.com/nltk/nltk/blob/develop/nltk/lm/vocabulary.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if an unknown word will be mapped to UNK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.')\n",
      "('language', 'is', 'never', '<UNK>', '.')\n"
     ]
    }
   ],
   "source": [
    "print(model.vocab.lookup(tokenized_text[0]))\n",
    "# If we lookup the vocab on unseen sentences not from the training data, \n",
    "# it automatically replace words not in the vocabulary with `<UNK>`.\n",
    "print(model.vocab.lookup('language is never unknownword .'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model provides a convenient interface to access counts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "6\n",
      "2\n",
      "2\n",
      "8839\n"
     ]
    }
   ],
   "source": [
    "print(model.counts['businessmen']) # i.e. Count('businessmen')\n",
    "print(model.counts[['businessmen']]['and']) # i.e. Count('and'|'businessmen')\n",
    "print(model.counts[['businessmen', 'and']]['officials']) # i.e. Count('officials'|'businessmen and')\n",
    "print(model.counts[[\"what\", \"the\"]]['economists']) # i.e. Count('economists'|'what the')\n",
    "print(model.counts[[start_symbol, start_symbol]]['The']) # i.e. Count('The'|'<s> <s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real purpose of training a language model is to have it score how probable words are in certain contexts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.732796436433447e-05\n",
      "0.10714285714285714\n",
      "0.3333333333333333\n",
      "0.043478260869565216\n",
      "0.08077900239439967\n"
     ]
    }
   ],
   "source": [
    "print(model.score('businessmen')) # P('businessmen')\n",
    "print(model.score('and', 'businessmen'.split()))  # P('and'|'businessmen')\n",
    "print(model.score('officials', 'businessmen and'.split()) ) # P('officials'|'businessmen and')\n",
    "print(model.score('economists', 'what the'.split()))  # P('economists'|'what the')\n",
    "print(model.score('The', '<s> <s>'.split()) ) # P('The'|'<s> <s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation using n-gram Language Model\n",
    "\n",
    "\n",
    "One feature of n-gram models is that they can be used to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feasibility', 'and', 'strategies', 'of', 'gaining', 'control', 'of', 'USAir', 'Group', \"'\", 's', 'acquisition', 'Portsmouth', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(60, random_seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a generate_sent function that generates a sentence based on a n-gram language model. We use [TreebankWordDetokenizer](https://kite.com/python/docs/nltk.treebank.TreebankWordDetokenizer) and apply some additional cleaning to the generated tokens in order to make the generated sentence more human-like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**TASK: Extend `generate_sent` so that the generated text sequence conditions on an existing text sequence (so that the generated text starts with a predefined word sequence).**</font>\n",
    "\n",
    "You might get inspiration from [here](https://www.nltk.org/api/nltk.lm.api.html#nltk.lm.api.LanguageModel.generate), i.e. the method `generate` has a parameter `text_seed` so that the generated text can be conditioned on preceding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :model: A n-gram language model from `nltk.lm.model`.\n",
    "    :num_words: Max no. of words to generate.\n",
    "    :random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    \n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == start_symbol:\n",
    "            continue\n",
    "        if token == end_symbol:\n",
    "            break\n",
    "        content.append(token)\n",
    "        \n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"feasibility and strategies of gaining control of USAir Group' s acquisition Portsmouth.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent(model, 60, random_seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
